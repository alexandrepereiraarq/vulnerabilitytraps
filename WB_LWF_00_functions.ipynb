{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from random import sample\n",
    "from pathlib import Path\n",
    "from scipy.stats import chi2_contingency\n",
    "#from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>definitions</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>import and cleanup Fn</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating functions that search parts of strings and clip values\n",
    "def left(aString, howMany):\n",
    "    if howMany <1:\n",
    "        return ''\n",
    "    else:\n",
    "        return aString[:howMany]\n",
    "\n",
    "def right(aString, howMany):\n",
    "    if howMany <1:\n",
    "        return ''\n",
    "    else:\n",
    "        return aString[-howMany:]\n",
    "\n",
    "def mid(aString, startChar, howMany):\n",
    "    if howMany < 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return aString[startChar:startChar+howMany]\n",
    "    \n",
    "def imports(choice = 1, ds = \"WB_LWF_dataset_v4.xlsx\"):\n",
    "    global floods\n",
    "    global questions\n",
    "    global randSample\n",
    "    \n",
    "    global newColumnsDict\n",
    "    global numerical\n",
    "    global numericalN\n",
    "    global years\n",
    "    global days\n",
    "    global hours\n",
    "    global currency\n",
    "    global quantity\n",
    "    global height\n",
    "    global multiple\n",
    "    global combined\n",
    "    global free\n",
    "    global combinedAlternatives\n",
    "    \n",
    "    file_path = Path('./IN/')\n",
    "    print(file_path)\n",
    "    \n",
    "    if choice == 1: #initial import\n",
    "        print(\"initial import\", choice)\n",
    "        file_name_floods = ds\n",
    "        file_name_questions = \"WB_LWF_questions.xlsx\"\n",
    "        floods = pd.read_excel(file_path / file_name_floods)#index_col=\"CampoComum\"\n",
    "        questions = pd.read_excel(file_path / file_name_questions)\n",
    "        print(\"Floods\", floods.info())\n",
    "        print('Questions: ', questions.info())\n",
    "        randSample = sorted(sample(range(len(floods.count(axis=1))),k=25))\n",
    "    if choice == 2: #data already prepared, just load secondary datasets for analysis\n",
    "        print(\"preped import\", choice)\n",
    "        file_name_floods = \"WB_LWF_floods_v26.xlsx\"\n",
    "        file_name_questions = \"WB_LWF_questions.xlsx\"\n",
    "        floods = pd.read_excel(file_path / file_name_floods)\n",
    "        questions = pd.read_excel(file_path / file_name_questions)\n",
    "        print(\"Floods\", floods.info())\n",
    "        print('Questions: ', questions.info())\n",
    "        randSample = sorted(sample(range(1455),k=25))\n",
    "    if choice == 3: #Kramer's matrix correlations\n",
    "        print(\"Kramer's matrix\", choice)\n",
    "        file_path = Path('./IN/')\n",
    "        global dEx5\n",
    "        file_name_floods = \"WB_LWF_floods_v26.xlsx\"\n",
    "        file_name_questions = \"WB_LWF_questions.xlsx\"\n",
    "        floods = pd.read_excel(file_path / file_name_floods)\n",
    "        questions = pd.read_excel(file_path / file_name_questions)\n",
    "        \n",
    "        numerical = questions[questions.question_type ==  'numerical']\n",
    "        years = questions[questions.question_subtype == 'years']\n",
    "        days = questions[questions.question_subtype == 'days']\n",
    "        hours = questions[questions.question_subtype == 'hours']\n",
    "        currency = questions[questions.question_subtype == 'currency']\n",
    "        quantity = questions[questions.question_subtype == 'quantity']\n",
    "        height = questions[questions.question_subtype == 'height']\n",
    "        numericalN = floods.columns[floods.columns.str.contains('N', regex=False)]\n",
    "        multiple = questions[questions.question_type == 'multiple answers']\n",
    "        combined = questions[questions.question_type == 'combined options']\n",
    "        combinedAlternatives = floods.columns[floods.columns.str.contains('op', regex=False)]\n",
    "        free = questions[questions.question_type == 'free']\n",
    "    if choice == 4: #Further analysis - post Stata\n",
    "        print(\"Post-Stata analysis\", choice)\n",
    "        questions_path = Path('./IN/')\n",
    "        floods_path = Path('./OUT/')\n",
    "        file_name_floods = \"WB_LWF_floods_v28.xlsx\"\n",
    "        file_name_questions = \"WB_LWF_questions.xlsx\"\n",
    "        floods = pd.read_excel(floods_path / file_name_floods)\n",
    "        questions = pd.read_excel(questions_path / file_name_questions)\n",
    "        \n",
    "        numerical = questions[questions.question_type ==  'numerical']\n",
    "        years = questions[questions.question_subtype == 'years']\n",
    "        days = questions[questions.question_subtype == 'days']\n",
    "        hours = questions[questions.question_subtype == 'hours']\n",
    "        currency = questions[questions.question_subtype == 'currency']\n",
    "        quantity = questions[questions.question_subtype == 'quantity']\n",
    "        height = questions[questions.question_subtype == 'height']\n",
    "        numericalN = floods.columns[floods.columns.str.contains('N', regex=False)]\n",
    "        multiple = questions[questions.question_type == 'multiple answers']\n",
    "        combined = questions[questions.question_type == 'combined options']\n",
    "        combinedAlternatives = floods.columns[floods.columns.str.contains('op', regex=False)]\n",
    "        free = questions[questions.question_type == 'free']\n",
    "        \n",
    "\n",
    "def defEx5():\n",
    "    global dataEx5\n",
    "    excludeList = (combined.question_code.tolist() + numericalN.tolist() + free.question_code.tolist())\n",
    "    dataEx5 = floods[[i for i in floods.columns if i not in (excludeList)]]\n",
    "    return dataEx5.head()\n",
    "\n",
    "def drops():\n",
    "    #dropping rows in which merge_data == \"master only (1)\"\n",
    "    global floods\n",
    "    f_merge_data = floods[floods.merge_data != \"master only (1)\"]\n",
    "    floods = f_merge_data\n",
    "    #removing unused columns from the dataframe. This includes questions of the 'free' type\n",
    "    excludeColList = (['identrevista','imei','memberid','A4','A6','merge_data', 'A34','A124','A215','A256', 'A257','A276','A277'])\n",
    "    floods = floods.drop(excludeColList, axis=1)\n",
    "    #filter lines that are not the first \"CampoComum\" in the list\n",
    "    \n",
    "    print(\"Floods\", floods.info())\n",
    "\n",
    "def questionCleanUp():\n",
    "    #removing letters from the column names\n",
    "    rList = np.array(floods.columns)\n",
    "    #cleaning answer numbers of their letters and replacing with the question numbers\n",
    "    for ii in range(len(rList)):\n",
    "        if (left(rList[ii],1) == 'A' or left(rList[ii],1) == 'B' ):\n",
    "            if len(rList[ii]) == 4:\n",
    "                rList[ii] = ('P'+ (right(rList[ii], (len(rList[ii])-1))))\n",
    "            elif len(rList[ii]) == 3:\n",
    "                rList[ii] = ('P0'+ (right(rList[ii], (len(rList[ii])-1))))\n",
    "            else:\n",
    "                rList[ii] = ('P00'+ (right(rList[ii], (len(rList[ii])-1))))\n",
    "    #print(rList)\n",
    "\n",
    "    #using the created list as column names for the answers dataframe\n",
    "    floods.columns = rList\n",
    "    floods.head()\n",
    "\n",
    "def change_formats(): \n",
    "    global floods\n",
    "    global newColumnsDict\n",
    "    global numerical\n",
    "    global years\n",
    "    global days\n",
    "    global hours\n",
    "    global currency\n",
    "    global quantity\n",
    "    global height\n",
    "    global multiple\n",
    "    global combined\n",
    "    numerical = questions[questions.question_type ==  'numerical']\n",
    "    years = questions[questions.question_subtype == 'years']\n",
    "    days = questions[questions.question_subtype == 'days']\n",
    "    hours = questions[questions.question_subtype == 'hours']\n",
    "    currency = questions[questions.question_subtype == 'currency']\n",
    "    quantity = questions[questions.question_subtype == 'quantity']\n",
    "    height = questions[questions.question_subtype == 'height']\n",
    "    multiple = questions[questions.question_type == 'multiple answers']\n",
    "    combined = questions[questions.question_type == 'combined options']\n",
    "    \n",
    "    #adding new columns based on question_subtype that supporting columns\n",
    "    newColumnsList = years.question_code.tolist() + days.question_code.tolist() + hours.question_code.tolist() + \\\n",
    "        quantity.question_code.tolist() + height.question_code.tolist() + currency.question_code.tolist()\n",
    "    \n",
    "    for ii in range(len(newColumnsList)):\n",
    "        newColumnsList[ii] = newColumnsList[ii] + 'N'\n",
    "\n",
    "    floods = floods.join(pd.DataFrame(np.nan, index=floods.index, columns=newColumnsList))\n",
    "\n",
    "    newColumnsDict = {'years': years.question_code.tolist(), 'days' : days.question_code.tolist(), \\\n",
    "                      'hours': hours.question_code.tolist()}\n",
    "    print(questions.head())\n",
    "\n",
    "#creating a function that goes into each column and replaces a given string\n",
    "def searchDestroy(columnName, replaceValues, multiplier):\n",
    "    newColumnName = columnName + 'N'\n",
    "    f = floods.loc[:, columnName].str.contains(replaceValues, na=False, regex=False)\n",
    "    if multiplier == 99999:\n",
    "        #using this as a flag for 'Irreparável' and 'Ainda não voltou ao normal'\n",
    "        #substitute the value for that column's max value\n",
    "        maxValue = str(floods.loc[:,newColumnName].max())\n",
    "        floods.loc[:, newColumnName][f] = pd.to_numeric(floods.loc[:, columnName][f].str.replace(replaceValues, maxValue), errors='coerce')\n",
    "    elif multiplier > 0:\n",
    "        floods.loc[:, newColumnName][f] = pd.to_numeric(floods.loc[:, columnName][f].str.replace(replaceValues, ''), errors='coerce') * multiplier\n",
    "    elif multiplier == 0:\n",
    "        floods.loc[:, newColumnName][f] = 0\n",
    "    \n",
    "def decombineAnswers(q):\n",
    "    global floods\n",
    "    global questions\n",
    "    m = int(questions.loc[questions.question_code == q,'answer_options'].str.count(' - ')) #number of available options/question\n",
    "    for ii in range(m):\n",
    "        #creating the new columns (as many as there are answer options)\n",
    "        #new columns have dummy value of 0 for 'false' (0 is Stata-friendly)\n",
    "        floods.loc[::,q+'op'+str(ii+1)] = 0\n",
    "    #B creating a dict for every possible answer\n",
    "    #B1 we need to determine which are the possible answers\n",
    "    possible_answers = questions[questions.question_code == q].answer_options.str.split(\";\", expand=True)\n",
    "    #B2 we need to find the actual text\n",
    "    #split results in a 1-line dataframe of series, we need to convert each series into a list \n",
    "    #from this 1-element list, we get the value (stored in index value 0) in it\n",
    "    new_dict_values = list(range(m))\n",
    "    for n in range(len(new_dict_values)):\n",
    "    #we also need to clean new_dict_values of \"n - \", as in \"10 - \" and of irregular use of punctuation ('.')\n",
    "        #print(possible_answers[n])\n",
    "        new_dict_values[n] = possible_answers[n].to_list()[0].split(' - ')[1].rstrip('\\.').rstrip()\n",
    "    #B3 we need to creat the keys, which are the 'm' columns created before\n",
    "    new_dict_keys = list(range(m))\n",
    "    for n in range(len(new_dict_keys)):\n",
    "        new_dict_keys[n] = str(q + 'op' + str(n+1))\n",
    "    #B4 we can now join the values and the keys in a dict\n",
    "    #in this dict, each key represents a column name and each value a possible answer\n",
    "\n",
    "    pQ_dict = {key: value for key, value in zip(new_dict_keys, new_dict_values)}\n",
    "    #B5 creating a function that goes into each column and replaces a given string\n",
    "    def dict_comparison(columnName, newColumnName):\n",
    "        matchValues = pQ_dict[newColumnName]    \n",
    "        f = floods.loc[:, columnName].str.contains(matchValues, na=False, regex=False)\n",
    "        #replacing those records that match the string with '1' (that is also Stata-friendly)\n",
    "        floods.loc[:, newColumnName][f] = 1\n",
    "    for ii in pQ_dict.keys():\n",
    "        dict_comparison(q,ii)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
